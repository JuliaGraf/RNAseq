\documentclass[12pt]{article}
\parindent 0pt
\parskip 10pt
\textwidth 15cm
\textheight 24cm
\oddsidemargin 15pt
\topmargin -60pt

\usepackage{booktabs}
\usepackage{hyperref}

%

\title{\textbf{RNA-seq pipeline}}
\author{Jessie Midgley, MSc Bioinformatics
\\ Jana Hoffmann, MSc Bioinformatics
\\ Julia Graf, MSc Bioinformatics}


\begin{document}
\maketitle



\section{Introduction}
\subsection{Pipelines}
A pipeline is a structured sequence of processes designed to carry out data analyses in a systematic and repeatable manner. It typically comprises a series of interconnected steps, including data input, preprocessing, algorithmic computation, and result generation. The use of pipelines allows for the automation of repetitive tasks, ensures the reproducibility of scientific experiments, and optimizes the execution of processes to run efficiently on different types of computing systems, such as servers, clusters, or cloud platforms.
\subsection{FAIR principles and reproducibility}
The FAIR principles offer a guideline to improve the findability, accessibility, interoperability and reusability of scientific data and workflows~\cite{wilkinson2016fair}.
Among other things, adding identifiers and extensive metadata to the data as well as using standardized communication protocols are part of the FAIR guiding principles. Meeting the requirements of the FAIR principles can assist to ensure reproducibility, which plays a major role in validating the results of scientific work.
\subsection{nf-core}
A major challenge in the bioinformatics community is achieving standardization, portability, and reproducibility of analysis pipelines.~\cite{ewels2020nf}. Many pipelines are built as custom in-house scripts that are closely integrated with the computing environment in which they were developed. This can present challenges for others attempting to reproduce the results, which is a fundamental requirement for scientific findings. The nf-core community project assembles a curated collection of best-practice analysis pipelines developed with Nextflow. Since nf-core pipelines are written in Nextflow, they can be run on various computational infrastructures, offer native support for container technologies like Docker and Singularity and allow for parallelization~\cite{di2017nextflow}. Pipelines within nf-core must adhere to strict guidelines, are required to provide high-quality documentation and must provide test dataset in order to run automated continuous-integration tests whenever there is a change to the pipeline~\cite{ewels2020nf}. A distinctive version tag is allocated with each pipeline release, thereby establishing a fixed link between the pipeline's implementation and the associated software dependencies. This way, nf-core assists in the development of high-quality pipelines that anyone can use.
\subsection{RNA-seq}
RNA-sequencing (RNA-seq) is a widely used technique for quantifying gene expression and allows for comprehensive profiling of the transcriptome \cite{Kukurba2015}.
One of the main goals of gene expression experiments is to identify transcripts that exhibit differential expression under various conditions.
A usual RNA-seq workflow involves isolating RNA from the sample, converting it to complementary DNA (cDNA), and sequencing it using an NGS platform.
The generated FastQ files contain sequencing reads and their associated base quality scores. These reads can then be aligned to a reference genome using mapping tools like STAR \cite{Dobin2012} or HISAT2 \cite{Kim2019}.
The alignment process allows for quantification of gene expression by counting the accumulation of aligned reads at specific loci in the reference genome \cite{Kukurba2015}. The resulting read counts reflect the transcript abundance, which is used to determine gene expression levels\cite{Kukurba2015}.
Alternatively, alignment-free quantification methods, such as Salmon \cite{Patro2017} or Kallisto \cite{Bray2016}, can be used to estimate transcript abundance without requiring a full read alignment.

\section{Methods}
\subsection{Structure of nf-core pipelines}
To ensure standardization, all nf-core pipelines have to be built using the nf-core template, which comprises many different files and folders. The most important files and directories are listed in Table~\ref{nfcorefiles}~\cite{nfcorefs}.
\begin{table}[h]
    \centering
    \caption{Important files and directories of an nf-core pipeline.}
    \begin{tabular}{@{}lp{9cm}@{}}
        \toprule
        \textbf{Filename/Directory Name} & \textbf{Description} \\ \midrule
        \verb|main.nf| & The main nextflow file which get executed when the pipeline is run. Calls workflows from the \verb|workflows/| directory. \\
        \verb|nextflow.config| & The main nextflow configuration file. It contains the default pipeline parameters, nextflow configuration options and defines different configuration profiles that can be used to run the pipeline. Imports files from the \verb|conf/| directory. \\
        \verb|README.md| & Contains basic information about the pipeline and usage. \\
        \verb|conf/| & Contains all of the configuration files. \\
        \verb|modules/| & Contains pipeline-specific and common nf-core modules. \\
        \verb|workflows/| & Contains the main pipeline workflows to be executed in the \verb|main.nf| file. \\
        \verb|subworkflows/| & Contains smaller subworkflows that typically consist out of a few modules chained together. \\ \bottomrule
    \end{tabular}
    \label{nfcorefiles}
\end{table}

The workflow outlines the entire flow of the pipeline, calling different processes and subworkflows, and ensuring they are executed in the correct order.
Subworkflows are smaller, reusable components within the main workflow. They should combine tools that work together as part of a specific subtask in the analysis~\cite{nfcorefs}.
A process represents a single, executable task in the pipeline. Each process usually runs a specific tool. These processes are defined in modules, which are independent, reusable Nextflow scripts. Modules allow for standardization and can be reused across different pipelines.
Channels pass data between these modules, ensuring that the outputs of one process connect to the inputs of another.


\subsection{Tools and computing environment}
The pipeline is implemented in Nextflow and utilises the tools listed in Table~\ref{table:1}.
\begin{table}[h]
    \centering
    \caption{Tools used in the pipeline and their version.}
    \label{table:1}
    \begin{tabular}{@{}lp{4cm}@{}}
        \toprule
        \textbf{Tool} & \textbf{Version} \\ \midrule
        FastQC & 0.12.1\\
        picard MarkDuplicates & 3.2.0-1-g3948afb6b\\
        RSEM & 1.3.1\\
        Salmon & 1.10.3\\
        STAR & 2.7.11b\\
        Trim Galore! & 0.6.10\\
        Cutadapt & 4.9\\
        RNAseq & v1.0.0\\
        Nextflow & 24.04.4\\
        \hline
    \end{tabular}
\end{table}
The pipeline automatically installs all required tools with Docker containers, ensuring that each tool runs in a consistent computing environment across all devices~\cite{docker}. Each process is executed within its own Docker container, and the Docker images used to build these containers are sourced from Seqera~\cite{seqera}. To maintain compatibility across both ARM and AMD architectures, users can specify their processor type, allowing the retrieval of the appropriate Docker images.
For testing, the pipeline was executed locally on our laptops.


\section{Results}
Our pipeline follows the essential steps of reading in a samplesheet, performing quality control, trimming and aligning reads to the reference genome, quantifying transcript abundance, and marking duplicate reads.
The samplesheet refers to a CSV file in which each row represents either a single-end FastQ file or two paired-end FastQ files, with a corresponding sample identifier.
Quality control is then performed on the sequencing files listed in the samplesheet, using the tool FastQC \cite{fastqc}. FastQC conducts a series of analyses on the raw sequencing data to give an overview of potential issues, presenting summary graphs and tables for easy visualisation.
Following quality control, the reads are trimmed with Trim Galore! \cite{trimgalore} to remove adapters and low-quality bases from the ends of the sequencing reads, resulting in compressed FastQ files.
The trimmed sequences are then aligned to the reference genome, producing compressed binary alignment map (BAM) files. This process is executed within a subworkflow that first utilizes STAR \cite{Dobin2012} to index the reference genome and subsequently aligns the reads to it. This subworkflow is divided into two processes, which allows for parallelization of the read alignment step.
Following alignment, the transcript quantification is performed in a second subworkflow consisting of three processes. First, reference transcripts are extracted from the genome by RSEM \cite{Li2011}, which prepares the reference by using a provided genome annotation file in gene transfer format (GTF).
In a second process, Salmon \cite{Patro2017} is used to index the reference transcripts generated by RSEM. Finally, the indexed reference transcripts, along with the sample reads, are processed by Salmon to produce transcript abundance tables in the third step of the alignment subworkflow. For each input sample, a TSV file is generated containing the transcript name, its length, its abundance in terms of Transcripts Per Million (TPM), and the estimated number of reads originating from each transcript.
The outputs of each process can be found in the \verb|results/| directory, under the corresponding subdirectory of the tool used by the process.
Additionally, each module in the workflow emits a \verb|versions.yml| file, containing the version number for each tool executed by the module. A summary of all software versions used by the pipeline can be found in \verb|results/pipeline_info/pipeline_software_versions.yml|.


\section{Discussion}
\subsection{Reproducibility and FAIR principles}
For every nf-core pipeline, it must be possible to run the pipeline with \verb|--profile| \verb|docker| and have all software requirements satisfied~cite{ewels2020nf}. Also, every container has to be versioned with a specific, stable version. This way, to reproduce an analysis that was conducted using a nf-core pipeline in a different computing environment, one only has to make sure to use the correct stable release tag. To find the matching nf-core pipeline for a specific use case, one can search for keywords on the nf-core website. All of the nf-core pipelines are open source and released under the MIT license which makes them accessible to everyone. Having a common repository for well-documented pipelines allows reusing pipelines rather than rewriting them.

Our pipeline also uses containerization with Docker and all of the containers have a stable version tag. The pipelines also creates a \verb|pipeline_software_versions.yml| file which contains the versions of all software used in the pipeline. Our pipeline is compatible with both ARM and AMD architectures. When run with the \verb|--arm| option, it leverages a different set of Docker containers than when executed with the \verb|--amd| option. As a result, cross-platform reproducibility cannot be fully guaranteed due to variations in the underlying computing environments. However, within the same architecture, reproducibility is ensured by using identical Docker containers to maintain a consistent execution environment. The different containers across platforms install the same version of the employed tools to minimize platform-specific differences. The source code for our pipeline is publicly available via GitHub and released under the MIT license which makes it accessible for everyone. Our documentation enables users that are not familiar with the pipeline to get started on using it.

\subsection{Outlook}
While the pipeline successfully generates transcript quantification data, there are opportunities to improve its performance by incorporating additional processes. For instance, integrating support for Unique Molecular Identifiers (UMIs) would allow for their extraction and downstream read deduplication, improving the accuracy of the results and reducing potential biases. The strandedness of sequencing reads could be inferred automatically by combining the subsampling of FastQ files with pseudoalignment, to determine whether reads correspond to the original RNA sequence or its complementary cDNA. Stranded RNA-Seq data offers advantages over non-stranded data, by improving the accuracy of transcript assembly and differential expression \cite{Signal2022}. Additionally, the removal of rRNA from reads can lead to the better detection of mRNA transcripts, which is important for the analysis of differential expression \cite{Pastor2022}. Finally, the generation of additional bigWig coverage files would allow for clearer visualisation of transcript coverage across the genome.

\section*{Data Availability}
The source code for this paper is available on \href{{https://github.com/JuliaGraf/RNAseq}}{GitHub}.

\newpage
\bibliographystyle{ieeetr}
\bibliography{bibliography}


\end{document}





