%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{animate}


\usepackage{tikz}
% Corporate Design of the University of TÃ¼bingen
% Primary Colors
\definecolor{TUred}{RGB}{165,30,55}
\definecolor{TUgold}{RGB}{180,160,105}
\definecolor{TUdark}{RGB}{50,65,75}
\definecolor{TUgray}{RGB}{175,179,183}

% Secondary Colors
\definecolor{TUdarkblue}{RGB}{65,90,140}
\definecolor{TUblue}{RGB}{0,105,170}
\definecolor{TUlightblue}{RGB}{80,170,200}
\definecolor{TUlightgreen}{RGB}{130,185,160}
\definecolor{TUgreen}{RGB}{125,165,75}
\definecolor{TUdarkgreen}{RGB}{50,110,30}
\definecolor{TUocre}{RGB}{200,80,60}
\definecolor{TUviolet}{RGB}{175,110,150}
\definecolor{TUmauve}{RGB}{180,160,150}
\definecolor{TUbeige}{RGB}{215,180,105}
\definecolor{TUorange}{RGB}{210,150,0}
\definecolor{TUbrown}{RGB}{145,105,70}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{caption} 
\captionsetup[table]{skip=10pt}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{nextflow RNAseq pipeline}

\begin{document}

\onecolumn
\icmltitle{nextflow RNAseq pipeline}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jana Hoffmann}{equal,first}
\icmlauthor{Julia Graf}{equal,second}
\icmlauthor{Jessie Midgley}{equal,third}
\end{icmlauthorlist}

% fill in your matrikelnummer, email address, degree, for each group member
\icmlaffiliation{first}{Matrikelnummer 5760486, jana2.hoffmann@student.uni-tuebingen.de, MSc Bioinformatics}
\icmlaffiliation{second}{Matrikelnummer 5656882, jul.graf@student.uni-tuebingen.de, MSc Bioinformatics}
\icmlaffiliation{third}{Matrikelnummer 6620875, jessie.midgley@student.uni-tuebingen.de, MSc Bioinformatics}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
TODO add our abstract
\end{abstract}

% This is the template for a figure from the original ICML submission pack. In lecture 10 we will discuss plotting in detail.
% Refer to this lecture on how to include figures in this text.
%
% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

\section{Introduction}
\subsection{Pipelines}
A pipeline is a structured sequence of processes designed to carry out data analyses in a systematic and repeatable manner. It typically comprises a series of interconnected steps, including data input, preprocessing, algorithmic computation, and result generation. The use of pipelines allows for the automation of repetitive tasks, ensures the reproducibility of scientific experiments, and optimizes the execution of processes to run efficiently on different types of computing systems, such as servers, clusters, or cloud platforms.
\subsection{FAIR principles and reproducibility}
The FAIR principles offer a guideline to improve the findability, accessibility, interoperability and reusability of scientific data and workflows~\cite{wilkinson2016fair}. 
Among other things adding identifiers and extensive metadata to the data as well as using standardized communication protocols are part of the FAIR guiding principles. Meeting the requirements of the FAIR principles can assist to ensure reproducibility, which plays a major role in validating the results of scientific work.
\subsection{nf-core}
A major challenge in the bioinformatics community is achieving standardization, portability, and reproducibility of analysis pipelines.~\cite{ewels2020nf}. Many pipelines are built as custom in-house scripts that are closely integrated with the computing environment in which they were developed. This can present challenges for others attempting to reproduce the results, which is a fundamental requirement for scientific findings. The nf-core community project assembles a curated collection of best-practice analysis pipelines developed with Nextflow. Since nf-core pipelines are written in Nextflow, they can be run on various computational infrastructures and offer native support for container technologies like Docker and Singularity. Pipelines within nf-core must adhere to strict guidelines, are required to provide high-quality documentation and must provide test dataset in order to run automated continuous-integration tests whenever there is a change to the pipeline. A distinctive version tag is allocated with each pipeline release, thereby establishing a fixed link between the pipeline's implementation and the associated software dependencies. This way, nf-core assists in the development of high-quality pipelines that anyone can use.
\subsection{RNA-seq}
RNA-sequencing (RNA-seq) is a widely used technique for quantifying gene expression and allows for comprehensive profiling of the transcriptome \cite{Kukurba2015}.
One of the main goals of gene expression experiments is to identify transcripts that exhibit differential expression under various conditions.
A usual RNA-seq workflow involves isolating RNA from the sample, converting it to complementary DNA (cDNA), and sequencing it using an NGS platform.
The generated FastQ files contain sequencing reads and their associated base quality scores. These reads can then be aligned to a reference genome using mapping tools like STAR \cite{Dobin2012} or HISAT2 \cite{Kim2019}.
The alignment process allows for quantification of gene expression by counting the accumulation of aligned reads at specific loci in the reference genome \cite{Kukurba2015}. The resulting read counts reflect the transcript abundance, which is used to determine gene expression levels, which is used to infer gene expression levels \cite{Kukurba2015}.
Alternatively, alignment-free quantification methods, such as Salmon \cite{Patro2017} or Kallisto \cite{Bray2016}, can be used to estimate transcript abundance without requiring a full read alignment.

\section{Methods}
\subsection{The file structure of nf-core pipelines}
To ensure standardization, all nf-core pipelines have to be built using the nf-core template, which comprises many different files and folders. The most important files and directories are listed in Table~\ref{nfcorefiles}~\cite{nfcorefs}.
\begin{table}[h]
    \centering
    \caption{Important files and directories of an nf-core pipeline.}
    \begin{tabular}{@{}lp{12cm}@{}}
        \toprule
        \textbf{Filename/Directory Name} & \textbf{Description} \\ \midrule
        \verb|main.nf| & The main nextflow file which get executed when the pipeline is run. Calls workflows from the \verb|workflows/| directory. \\
        \verb|nextflow.config| & The main nextflow configuration file. It contains the default pipeline parameters, nextflow configuration options and defines different configuration profiles that can be used to run the pipeline. Imports files from the \verb|conf/| directory. \\
        \verb|README.md| & Contains basic information about the pipeline and usage. \\
        \verb|conf/| & Contains all of the configuration files. \\
        \verb|modules/| & Contains pipeline-specific and common nf-core modules. \\
        \verb|workflows/| & Contains the main pipeline workflows to be executed in the \verb|main.nf| file. \\
        \verb|subworkflows/| & Contains smaller subworkflows that typically consist out of a few modules chained together. \\ \bottomrule
    \end{tabular}
    \label{nfcorefiles}
\end{table}

The workflow outlines the entire flow of the pipeline, calling different processes and subworkflows, and ensuring they are executed in the correct order.
Subworkflows are smaller, reusable components within the main workflow. They should combine tools that work together as part of a specific subtask in the analysis~\cite{nfcorefs}.
A process represents a single, executable task in the pipeline. Each process usually runs a specific tool. These processes are defined in modules, which are independent, reusable Nextflow scripts. Modules allow for standardization and can be reused across different pipelines. 
Channels pass data between these modules, ensuring that the outputs of one process connect to the inputs of another.


\subsection{Where / how did you compute, which packages etc did you use (be FAIR when describing}
The pipeline is implemented in Nextflow and utilises the tools listed in table~\ref{table:1}.
\begin{table}[h]
    \centering
    \caption{Tools used in the pipeline and their version.}
    \label{table:1}
    \begin{tabular}{@{}lp{4cm}@{}}
        \toprule
        \textbf{Tool} & \textbf{Version} \\ \midrule
        FastQC & 0.12.1\\
        picard MarkDuplicates & 3.2.0-1-g3948afb6b\\
        RSEM & 1.3.1\\
        Salmon & 1.10.3\\
        STAR & 2.7.11b\\
        Trim Galore! & 0.6.10\\
        Cutadapt & 4.9\\
        RNAseq & v1.0.0\\
        Nextflow & 24.04.4\\
        \hline
    \end{tabular}
\end{table}
The pipeline automatically installs all required tools by utilizing Docker containers, ensuring that each tool runs in a consistent computing environment across all devices~\cite{docker}. Each process is executed within its own Docker container, and the Docker images used to build these containers are sourced from Seqera~\cite{seqera}. To maintain compatibility across both ARM and AMD architectures, users can specify their processor type, allowing the retrieval of the appropriate Docker images.
For testing, the pipeline was executed locally on our laptops.


\section{Results}
Our pipeline follows the essential steps of reading in a samplesheet, performing quality control, trimming and aligning reads to the reference genome, quantifying transcript abundance, and marking duplicate reads.
The samplesheet refers to a CSV file in which each row represents either a single-end FastQ file or two paired-end FastQ files, with a corresponding sample identifier.
Quality control is then performed on the sequencing files listed in the samplesheet, using the tool FastQC \cite{fastqc}. FastQC conducts a series of analyses on the raw sequencing data to give an overview of potential issues, presenting summary graphs and tables for easy visualisation.
Following quality control, the reads are trimmed with Trim Galore! \cite{trimgalore} to remove adapters and low-quality bases from the ends of the sequencing reads, resulting in compressed FastQ files.
The trimmed sequences are then aligned to the reference genome, producing compressed binary alignment map (BAM) files. This process is executed within a subworkflow that first utilizes STAR \cite{Dobin2012} to index the reference genome and subsequently aligns the reads to it. This subworkflow is divided into two processes, which allows for parallelization of the read alignment step.
Following alignment, the transcript quantification is performed in a second subworkflow consisting of three processes. First, reference transcripts are extracted from the genome by RSEM \cite{Li2011}, which prepares the reference by using a provided genome annotation file in GTF format.
In a second process, Salmon \cite{Patro2017} is used to index the reference transcripts generated by RSEM. Finally, the indexed reference transcripts, along with the sample reads, are processed by Salmon to produce transcript abundance tables in the third step of the alignment subworkflow. For each input sample, a TSV file is generated containing the transcript name, its length, its abundance in terms of Transcripts Per Million (TPM), and the estimated number of reads originating from each transcript.
The outputs of each process can be found in the \verb|results/| directory, under the corresponding subdirectory of the tool used by the process.
Additionally, each module in the workflow emits a \verb|versions.yml| file, containing the version number for each tool executed by the module. A summary of all software versions used by the pipeline can be found in \verb|results/pipeline_info/pipeline_software_versions.yml|.

\subsection{potentially describe aspects like speed with respect to parallelization (potentially compare for a couple of files to running plain bash...)}
\section{Discussion}
\subsection{How does nf-core achieve reproducibility and FAIR pipelines with focus on what your pipeline would still need to be able to be FAIR, reprod.}
For every nf-core pipeline, it must be possible to run the pipeline with \verb|--profile docker| and have all software requirements satisfied. Also, every container has to be versioned with a specific, stable version. This way, to reproduce an analysis that was conducted using a nf-core pipeline in a different computing environment, one only has to make sure to use the correct stable release tag. To find the matching nf-core pipeline for a specific use case, one can search for keywords on the nf-core website. All of the nf-core pipelines are open source and released under the MIT license which makes them accessible to everyone. Having a common repository for well-documented pipelines allows reusing pipelines rather than rewriting them.

Our pipeline also uses containerization with Docker and all of the containers have a stable version tag. The pipelines also creates a \verb|pipeline_software_versions.yml| file which contains the versions of all software used in the pipeline. However, our pipeline distinguishes between the ARM and the AMD architecture. Running the pipeline with \verb|--arm| makes use of different Docker containers than running it with the \verb|--amd| option which is why reproducibility can only be achieved on the same architecture, but not across different architectures. The source code for our pipeline is publicly available via GitHub and released under the MIT license which makes it accessible for everyone. Our comprehensive documentation enables users that are not familiar with the pipeline to get started on using it.
\subsection{What is missing to your pipeline to be a strong RNA-seq workflow. what would be the next steps - Outlook}
While the pipeline successfully generates transcript quantification data, there are opportunities to improve its performance by incorporating additional processes. For instance, integrating support for Unique Molecular Identifiers (UMIs) would allow for their extraction and downstream read deduplication, improving the accuracy of the results and reducing potential biases. The strandedness of sequencing reads could be inferred automatically by combining the subsampling of FastQ files with pseudoalignment, to determine whether reads correspond to the original RNA sequence or its complementary cDNA. Stranded RNA-Seq data offers advantages over non-stranded data, by improving the accuracy of transcript assembly and differential expression \cite{Signal2022}. Additionally, the removal of rRNA from reads can lead to the better detection of mRNA transcripts, which is important for the analysis of differential expression \cite{Pastor2022}. Finally, the generation of additional bigWig coverage files would allow for clearer visualisation of transcript coverage across the genome.

\section*{Data Availability}
The source code for this paper is available on \href{{https://github.com/JuliaGraf/RNAseq}}{GitHub}.

\bibliography{bibliography}
\bibliographystyle{icml2023}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
